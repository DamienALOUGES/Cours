# -*- coding: utf-8 -*-
"""TD7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMxun7pRHi3SBjxXWzeuvenNCUswHlfc
"""

# Commented out IPython magic to ensure Python compatibility.
try:
#   %tensorflow_version 2.x
except Exception:
  pass

import tensorflow as tf
tf.test.gpu_device_name()

from tensorflow import keras
import pandas as pd
import io
from google.colab import files
import numpy as np

#importation of train files
uploaded = files.upload()

train_set=pd.read_csv(io.BytesIO(uploaded['letter_recognition_train_4.csv']),sep=',',header=None)

train_set

from keras.utils import to_categorical
x=train_set.iloc[:,1:]
y=train_set.iloc[:,0]

print(x)
print(y)

#transformation of all letters of y into integers from 0 to 25 to apply to_categorical function
for i in range (len (y)):
  y[i]=ord(y[i]) - 65

print(y)

#use of to_categorical function on y
trainy = to_categorical(y)
trainy

#here i convert alll values of trainx to float32 t0 fit the model
trainx=np.array(x)
trainx=trainx.astype(np.float32)
trainx.dtype

#creation of the model
#we have 16 input from the dataset and 26 output classes
#so the model structure is 16 input layers, 50 hidden layers and 26 output layers
model= keras.Sequential([keras.layers.Dense(16),
                         keras.layers.Dense(50, activation='relu'),
                         keras.layers.Dense(26, activation='softmax')
                         ])

from keras.optimizers import SGD
lrate=0.01
model.compile(loss='categorical_crossentropy',
              optimizer=keras.optimizers.SGD(learning_rate=lrate), metrics=['accuracy'])

#importation of the test set
uploaded = files.upload()

test_set=pd.read_csv(io.BytesIO(uploaded['letter-recognition_test.csv']),sep=',',header=None)

testx=test_set.iloc[:,1:]
testy=test_set.iloc[:,0]

for i in range (len (testy)):
  testy[i]=ord(testy[i]) - 65

testy = to_categorical(testy)

testx=np.array(testx)
testx=testx.astype(np.float32)

history=model.fit(trainx, trainy, validation_data=(testx, testy),epochs=300, verbose=0)

#plot of the accuracy of  the train set and the test set with the learningrate in red and blue
plt.plot(history.history['acc'], label='train',color = 'r')
plt.plot(history.history['val_acc'], label='test', color = 'b')
plt.title('lrate='+str(lrate), pad=-50)

#plot of the loss of  the train set and the test set in yellow and green
plt.plot(history.history['loss'], label='train',color = 'g')
plt.plot(history.history['val_loss'], label='test', color = 'y')


#and print of finales values of test accuracy and loss
print("final test loss: "+ str(history.history['val_loss'][-1]))
print("final test accuracy: "+ str(history.history['val_acc'][-1]))

"""With a learning rate of 0.01 we got an accuracy that is over 0.9 that is quite good but not perfect,
so i try to go with a learning rate of 0.001 and got a worse accuracy so the best i got is  with lrate=0.01
"""